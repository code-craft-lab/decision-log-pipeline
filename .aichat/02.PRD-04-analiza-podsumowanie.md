<conversation_summary>

<decisions>
1. Format logów: JSON array; pola nieistotne dla MVP.  
2. OPA wysyła batch co 60-120 s (lub szybciej); średnio 15 000 logów/min, szczyt 50 000/min; maks. 200 mln logów/miesiąc.  
3. Jedna decyzja: 2,5-3,5 kB; maks. rozmiar rekordu Kafka: 1 MB.  
4. Logi muszą trafić do OpenSearch w ≤ 1 min od wysłania przez OPA.  
5. Serwis HTTP potwierdza przyjęcie dopiero po publikacji batcha do Kafka.  
6. Kafka otrzymuje surowy JSON array; klucz partycji dowolny.  
7. Konsument Kafka zapisuje do S3 (gzip, ścieżka `s3://bucket/environment/product/`); retencja 20 lat; brak wersjonowania prefixu.  
8. Konsument Kafka indeksuje do OpenSearch; retencja 3 dni z automatycznym usunięciem.  
9. DLQ dla błędów zapisu; obsługa ręczna przez zespół projektu; dashboard/alerty zapewnia platforma.  
10. Bezpieczeństwo: uwierzytelnienie API Key (wiele kluczy w K8s Secrets); brak dodatkowego szyfrowania danych at-rest.  
11. Maskowanie/PII – brak w MVP.  
12. Metryki i obserwowalność dostarcza istniejąca platforma (poza zakresem projektu).  
13. OpenSearch cluster i infrastruktura poza zakresem projektu.
</decisions>

<matched_recommendations>
1. Zaplanować chunking po stronie producenta, gdy batch przekroczy limit 1 MB.
2. Zapewnić wsparcie wielu API Keys dla komunikacji pomiędzy OPA i producentem.
3. Zdefiniować playbook ręcznego re-process DLQ oraz SLO czasu reakcji.
</matched_recommendations>

<prd_planning_summary>
a. Główne wymagania funkcjonalne  
• Serwis HTTP przyjmujący batche decyzji z OPA (API Key auth, obsługa > 50 000 logów/min).  
• Publikacja batchy do Apache Kafka (≤ 1 MB).  
• Konsument 1: zapis do S3 w formacie gzip; trwałość danych 20 lat.  
• Konsument 2: indeksacja do OpenSearch; dostępność danych ≤ 60 s; automatyczne usunięcie po 3 dniach.  
• Mechanizm DLQ przenoszący problematyczne wiadomości; ręczna obsługa.

b. Kluczowe historie użytkownika / ścieżki  
• Operator bezpieczeństwa przegląda log w OpenSearch w ciągu minuty od zdarzenia.  
• Inżynier SRE odzyskuje utracone logi, pobierając batch z DLQ i ponownie przetwarzając do S3/OpenSearch.  
• System archiwizacji pobiera pliki gzip z S3 po dowolnym czasie do 20 lat wstecz.

c. Kryteria sukcesu i metryki  
• ≥ 99,99 % decyzji zapisanych w S3 (metryka write-success).  
• ≥ 99,99 % decyzji z ostatnich 3 dni dostępnych w OpenSearch w ≤ 60 s (metryka age).  
• Zero utraconych batchy podczas szczytu 50 000 logów/min.  
• DLQ = 0 nieprzetworzonych wiadomości > 24 h.

d. Nierozwiązane kwestie  
• Parametry rollover bundlingu gzip (czas vs. rozmiar).  
• Formalny playbook obsługi DLQ (SLO, procedura eskalacji).
</prd_planning_summary>

<unresolved_issues>
1. Ustalenie dokładnej strategii bundlingu gzip oraz harmonogramu rollover.
2. Projekt i implementacja wielu API Keys.
3. Opracowanie szczegółowego playbooka dla DLQ (odpowiedzialności, czas reakcji).
</unresolved_issues>

</conversation_summary>